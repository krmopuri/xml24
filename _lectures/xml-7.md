---
type: lecture
date: 2024-08-30
title: (xml-7) Interpreting Neural Networks (Pixel Attribution)

# optional
# please use /static_files/notes directory to store notes
# thumbnail: /static_files/path/to/image.jpg

# optional
tldr: "Highlighting the input features (e.g., pixels) relevant for a DNN prediction."
  
# optional
# set it to true if you don't want this lecture to appear in the updates section
hide_from_announcments: false

# optional
links: 
    #- url: /static_files/presentations/lec.zip
    #  name: notes
    #- url: /static_files/presentations/code.zip
    #  name: codes
    - url: https://docs.google.com/presentation/d/1i92FyZow4PItJ65V0ctLJjk5nM_zZxwdNwAlLKAtdOc/edit?usp=sharing
      name: slides
    #- url: /static_files/presentations/lec.zip
    #  name: other
---

**Suggested Readings:**
- [Visualising image classification models and saliency maps (ICLRW-2012)](https://arxiv.org/abs/1312.6034)
- [Visualizing and understanding CNNs (ECCV-2013)](https://arxiv.org/abs/1311.2901)
- [SmoothGrad (2017)](https://3dvar.com/Smilkov2017SmoothGrad.pdf)
- [Interpreting DNNs is fragile (AAAI-2019)](https://arxiv.org/abs/1710.10547)
- [Sanity Checks for Saliency Maps (NeurIPS-2018)](https://arxiv.org/pdf/1810.03292)
- [The (un) reliability of saliency methods (Explainable AI-2019)](https://arxiv.org/pdf/1711.00867)
- [Sanity Checks for Saliency Metrics (AAAI-2020)](https://ojs.aaai.org/index.php/AAAI/article/view/6064)

